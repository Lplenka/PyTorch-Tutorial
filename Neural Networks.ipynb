{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Next Generation Weapon](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/pytorch-logo-flat-300x210.png)\n",
    "\n",
    "        \n",
    "* [**Section : 1. Neural Network**](#Section-:-2.-Neural-Network)\n",
    "     * [Linear Regression](#Linear-Regression)\n",
    "     * [Relationship Fitting Regression Model](#Relationship-Fitting-Regression-Model)\n",
    "     * [Distinguish type classification](#Distinguish-type-classification)\n",
    "     * [Easy way to Buid Neural Network](#Easy-way-to-Buid-Neural-Network)\n",
    "     * [Save and Reload Model](#Save-and-Reload-Model)\n",
    "     * [Train on Batch](#Train-on-batch)\n",
    "     * [Optimizers](#Optimizers)\n",
    " \n",
    "* [**Section : 2. Advance Neural Network**](#Section-:-3.-Advance-Neural-Network)\n",
    "    * [CNN](#CNN)\n",
    "    * [RNN-Classification](#RNN-Classification)\n",
    "    * [RNN-Regression](#RNN-Regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section : 1. Neural Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Linear Regression\n",
    "\n",
    "---\n",
    "\n",
    "* It is worth underlining that this is an example focused on re-applying the techniques introduced. Indeed, PyTorch offers much more advanced methodologies to accomplish the same task, introduced in the following tutorials.\n",
    "* In this example we will consider a simple one-dimensional synthetic problem (with some added noise):\n",
    "Example : \n",
    "![](https://i.pinimg.com/564x/cd/6e/c7/cd6ec7106cfd22cdfd5c542a84a8b869.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(30, 1)*2.0 #data points\n",
    "w = np.random.rand(2, 1)\n",
    "y = X*w[0] + w[1] + np.random.randn(30, 1) * 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to detect the line's coefficient, we define a linear model:\n",
    "#### Define Weight and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "W = Variable(torch.rand(1, 1), requires_grad=True)\n",
    "b = Variable(torch.rand(1), requires_grad=True)\n",
    "\n",
    "def linear(x):\n",
    "    return torch.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using `torch.matmul` is redundant in this case, but we want the function to be as general as possible to be re-used for more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need to cast the dataset from NumPy to PyTorch (32bit) using float().\n",
    "* The cost function is a mean squared error.\n",
    "* After the back-propagation step we update the weights with a gradient descent, and we explicitly use W.data instead of W, in order not to override the original variables\n",
    "* At the end of every iteration, gradients are reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = Variable(torch.from_numpy(X)).float()\n",
    "yt = Variable(torch.from_numpy(y)).float()\n",
    "\n",
    "for epoch in range(2500):\n",
    "\n",
    "    # Compute predictions\n",
    "    y_pred = linear(Xt)\n",
    "\n",
    "    # Compute cost function\n",
    "    loss = torch.mean((y_pred - yt) ** 2)\n",
    "\n",
    "    # Run back-propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update variables\n",
    "    W.data = W.data - 0.005*W.grad.data\n",
    "    b.data = b.data - 0.005*b.grad.data\n",
    "\n",
    "    # Reset gradients\n",
    "    W.grad.data.zero_()\n",
    "    b.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After Training we can see the graph like this\n",
    "![](https://iaml.it/blog/fun-with-pytorch-part-1/images/regressione_lineare_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
